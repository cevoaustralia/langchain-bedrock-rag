{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6715bc2b",
   "metadata": {},
   "source": [
    "# Vector Similarity Astra-Bedrock Search QA Quickstart\n",
    "\n",
    "Set up a simple Question-Answering system with LangChain and Amazon Bedrock, using Astra DB as the Vector Database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761d9b70",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Make sure you have a vector-capable Astra database (get one for free at [astra.datastax.com](https://astra.datastax.com)):\n",
    "\n",
    "- Astra DB\n",
    "    - an **Access Token** for your database with role _Database Administrator_ (see [here](https://awesome-astra.github.io/docs/pages/astra/create-token/) for details).\n",
    "    \n",
    "- Amazon Web Services\n",
    "    - an Identity with access to **Amazon Bedrock**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c840842",
   "metadata": {},
   "source": [
    "## Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b243d1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import warnings\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import boto3\n",
    "import cassio\n",
    "from litellm import completion\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.vectorstores import Cassandra\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccce0c2",
   "metadata": {},
   "source": [
    "## Astra DB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b0a92b-f9ce-4810-a8ef-5741b2449b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "## set ENV variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"openai key\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"cohere key\"\n",
    "\n",
    "ASTRA_DB_ID = os.environ[\"ASTRA_DB_ID\"]\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"]\n",
    "ASTRA_VECTOR_ENDPOINT = os.environ[\"ASTRA_VECTOR_ENDPOINT\"]\n",
    "#ASTRA_DB_KEYSPACE = \"blueillusion\"\n",
    "#ASTRA_DB_COLLECTION = \"sydshakespeare\"\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"blueillusion\"\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a066f378-8fdb-4d4b-a7b1-bf685fbfd413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cassio.init(\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    database_id=ASTRA_DB_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c674f1",
   "metadata": {},
   "source": [
    "## AWS Credentials Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219cb02-f955-4fb3-85af-3f149868958a",
   "metadata": {},
   "source": [
    "These are set as environment variables for usage by the subsequent `boto3` calls. Please refer to [boto3's documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) on the possible ways to supply your credentials in a more production-like environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb76ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "AWS_SECRET_ACCESS_KEY = os.environ[\"AWS_SECRET_ACCESS_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb0639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_bedrock_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if runtime:\n",
    "        service_name='bedrock-runtime'\n",
    "    else:\n",
    "        service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388ac1d",
   "metadata": {},
   "source": [
    "## Set up AWS Bedrock objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d65c46f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using profile: AWSAdministratorAccess-590312749310\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bedrock_runtime = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region='us-east-1' #os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\",\n",
    "    client=bedrock_runtime\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d9f48b",
   "metadata": {},
   "source": [
    "## Set up the Vector Store\n",
    "\n",
    "This command will create a suitable table in your database if it does not exist yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d9f48c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store = Cassandra(\n",
    "    embedding=bedrock_embeddings,\n",
    "    table_name=\"sydshakespeare\",\n",
    "    session=None,  # <-- meaning: use the global defaults from cassio.init()\n",
    "    keyspace=None,  # <-- meaning: use the global defaults from cassio.init()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af24199",
   "metadata": {},
   "source": [
    "## Populate the database\n",
    "\n",
    "Add lines for the text of \"Romeo and Astra\", Scene 5, Act 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dab5114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file texts already exists.\n",
      "Error occurred while processing: texts.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 75985  100 75985    0     0   212k      0 --:--:-- --:--:-- --:--:--  213k\n"
     ]
    }
   ],
   "source": [
    "# retrieve the text of a scene from act 5 of Romeo and Astra. \n",
    "# Juliet's name was changed to Astra to prevent the LLM from \"cheating\" when providing an answer.\n",
    "! mkdir -p \"texts\"\n",
    "! curl \"https://raw.githubusercontent.com/awesome-astra/docs/main/docs/pages/aiml/aws/bedrock_resources/romeo_astra.json\" \\\n",
    "    --output \"texts/romeo_astra.json\"\n",
    "input_lines = json.load(open(\"texts/romeo_astra.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6b732",
   "metadata": {},
   "source": [
    "Next, you'll populate the database with the lines from the play.\n",
    "This can take a couple of minutes, please be patient.  In total there are 321 lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bae5520",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 321 documents ... Done.\n"
     ]
    }
   ],
   "source": [
    "input_documents = []\n",
    "\n",
    "for input_line in input_lines:\n",
    "    if (input_line[\"ActSceneLine\"] != \"\"):\n",
    "        (act, scene, line) = input_line[\"ActSceneLine\"].split(\".\")\n",
    "        location = \"Act {}, Scene {}, Line {}\".format(act, scene, line)\n",
    "        metadata = {\"act\": act, \"scene\": scene, \"line\": line}\n",
    "    else:\n",
    "        location = \"\"\n",
    "        metadata = {}\n",
    "    quote_input = \"{} : {} : {}\".format(location, input_line[\"Player\"], input_line[\"PlayerLine\"])\n",
    "    input_document = Document(page_content=quote_input, metadata=metadata)\n",
    "    input_documents.append(input_document)\n",
    "    \n",
    "print(f\"Adding {len(input_documents)} documents ... \", end=\"\")\n",
    "vector_store.add_documents(documents=input_documents, batch_size=50)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162c2d1-188e-43f0-b1c3-342b80641060",
   "metadata": {},
   "source": [
    "## Answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5332b838-6c1f-40f4-a29e-2b2d0250f408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template_str = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67346b-d5ca-433d-858e-5c21397f9de5",
   "metadata": {
    "tags": []
   },
   "source": [
    "We choose to use the following LLM model (see [this page](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html#model-parameters-general) for more info):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee8b2ee0-f9bf-4ada-8fde-7d917d89c6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081fd78-1710-4a42-a942-a14f652c854d",
   "metadata": {},
   "source": [
    "Here the question-answering function is set up, implementing the RAG pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e82ef49-ffec-4429-bcc2-ea09f50333cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "req_accept = \"application/json\"\n",
    "req_content_type = \"application/json\"\n",
    "\n",
    "# This, created from the vector store, will fetch the top relevant documents given a text query\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "def answer_question(question: str, verbose: bool = False) -> str:\n",
    "    if verbose:\n",
    "        print(f\"\\n[answer_question] Question: {question}\")\n",
    "    # Retrieval of the most relevant stored documents from the vector store:\n",
    "    context_docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join(doc.page_content for doc in context_docs)\n",
    "    if verbose:\n",
    "        print(\"\\n[answer_question] Context:\")\n",
    "        print(context)\n",
    "    # Filling the prompt template with the current values\n",
    "    llm_prompt_str = prompt.format(\n",
    "        question=question,\n",
    "        context=context,\n",
    "    )\n",
    "    # Invocation of the Amazon Bedrock LLM for text completion -- ultimately obtaining the answer\n",
    "    llm_body = json.dumps({\"prompt\": llm_prompt_str, \"max_tokens_to_sample\": 500})\n",
    "    llm_response = bedrock_runtime.invoke_model(\n",
    "        body=llm_body,\n",
    "        modelId=model_id,\n",
    "        accept=req_accept,\n",
    "        contentType=req_content_type,\n",
    "    )\n",
    "    llm_response_body = json.loads(llm_response[\"body\"].read())\n",
    "    answer = llm_response_body[\"completion\"].strip()\n",
    "    if verbose:\n",
    "        print(f\"\\n[answer_question] Answer: {answer}\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "599d8856-921e-4bf4-8979-ff54b13de6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Based on the provided context, it seems that Astra and Tybalt are characters who die in the story. The lines mention Astra being found dead, bleeding and warm, and refer to Tybalt's \"untimely death\". The Prince also says someone came to the vault to die and lie with Astra, implying she is dead. So Astra and Tybalt are two characters who die, according to this context.\n"
     ]
    }
   ],
   "source": [
    "my_answer = answer_question(\"Who dies in the story?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(my_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71cc88-61e3-42e5-802b-4ba4eaa795b4",
   "metadata": {},
   "source": [
    "Let's take a look at the RAG process piece-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fbeaf2d-f589-4d04-8447-4b876375f5b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[answer_question] Question: Who dies in the story?\n",
      "\n",
      "[answer_question] Context:\n",
      "Act 5, Scene 3, Line 184 : First Watchman : And Astra bleeding, warm, and newly dead,\n",
      "Act 5, Scene 3, Line 184 : First Watchman : And Astra bleeding, warm, and newly dead,\n",
      "Act 5, Scene 3, Line 244 : FRIAR LAURENCE : Was Tybalt's dooms-day, whose untimely death\n",
      "Act 5, Scene 3, Line 300 : PRINCE : Came to this vault to die, and lie with Astra.\n",
      "Act 5, Scene 3, Line 300 : PRINCE : Came to this vault to die, and lie with Astra.\n",
      "\n",
      "[answer_question] Answer: Based on the provided context, it seems that Astra and Tybalt are characters who die in the story. The lines mention Astra being found dead, and refer to Tybalt's \"untimely death\".\n",
      "\n",
      "============================================================\n",
      "Based on the provided context, it seems that Astra and Tybalt are characters who die in the story. The lines mention Astra being found dead, and refer to Tybalt's \"untimely death\".\n"
     ]
    }
   ],
   "source": [
    "my_answer = answer_question(\"Who dies in the story?\", verbose=True)\n",
    "print(\"=\" * 60)\n",
    "print(my_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ba03a-66d1-47fe-9a8e-6b2713ddd0f9",
   "metadata": {},
   "source": [
    "### Interactive QA session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fe0df9c-b24d-4f35-aee4-8bef2dd1e1a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer ==> Based on the context provided, Juliet is Romeo's faithful wife who is found dead and appears to have committed violence on herself.\n",
      "Answer ==> Based on the provided context, I don't know who Romeo is referring to in these lines. The context provided does not give enough information to determine who Romeo is talking about.\n",
      "Answer ==> Based on the provided context, I do not know who \"tyblah\" is. The context includes lines spoken by the Prince, Page, and Paris, but does not mention anyone named \"tyblah\". Without more context about this character, I cannot determine who they are.\n",
      "Answer ==> Based on the context provided, Tybalt is a character who was killed in an \"untimely death\" prior to the events described in the lines from Friar Laurence. Friar Laurence mentions that Astra pined for someone other than Tybalt, implying that Tybalt is a separate individual who is now deceased.\n",
      "[User, AI exeunt]\n"
     ]
    }
   ],
   "source": [
    "user_question = \"\"\n",
    "while True:\n",
    "    user_question = input(\"Enter a question (empty to quit):\").strip()\n",
    "    if user_question:\n",
    "        print(f\"Answer ==> {answer_question(user_question)}\")\n",
    "    else:\n",
    "        print(\"[User, AI exeunt]\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c2d97",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "To learn more about Amazon Bedrock, visit this page: [Introduction to Amazon Bedrock](https://github.com/aws-samples/amazon-bedrock-samples/tree/main/introduction-to-bedrock)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
